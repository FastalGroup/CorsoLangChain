{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVZkAFD3xvHLTVrjXSesUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FastalGroup/CorsoLangChain/blob/main/AgenteRAG_CorsoBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realizzazione di un Agente ChatBot RAG\n",
        "\n",
        "In questo notebook viene descritta la costruzione di un **Agente Intelligente Conversazionale** che è in grado di assistere un utente relativamente ad uno specifico dominio applicativo.\n",
        "\n",
        "Il ChatBot sfrutta le capacità di comprensione del linguaggio naturale di un Modello di IA, **Large Language Model**, di ultima generazione per comprendere le richieste dell'utente, reperire le informazioni necessarie da un database che contiene documenti specifici sul dominio applicativo (**base di conoscenza**), e rispondere in modo appropriato all'utente.\n",
        "\n"
      ],
      "metadata": {
        "id": "YBST55e4s7Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concetti generali\n",
        "\n",
        "L'assistente descritto e realizzato in questo notebook si avvale di strumenti e tecnologie che, al momento, costituiscono lo stato dell'arte nel settore dell'**Intelligenza Artificiale Generativa**.\n",
        "\n",
        "Nel seguito sono descritti i componenti e i paradigmi utilizzati nell'architettura del ChatBot.\n",
        "\n"
      ],
      "metadata": {
        "id": "07uLIJ7FwRqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM - Large Language Model\n",
        "\n",
        "Il LLM è il componente centrale dell'architettura del nostro assistente.\n",
        "\n",
        "Un LLM è una **rete neurale** di grandi dimensioni, comprendente diverse centinaia di miliardi di interconnessioni tra nodi di elaborazione (neuroni), strutturate secondo un \"*modello*\" derivato dalla interconnessione di strutture chiamate \"*transformer*\".\n",
        "\n",
        "I primi **LLM** presentati dai grandi provider come **OpenAI** e **Google** erano sistemi in grado di generare risposte in linguaggio naturale, corrette dal punto di vista grammaticale, sintattico e semantico, in risposta ad un testo ricevuto come input.\n",
        "\n",
        "A partire dal 2022, i modelli presentati e offerti come servizio sul mercato, hanno una struttura e una funzionalità molto più complessa, orientata ad interazioni di tipo conversazionale.\n",
        "\n",
        "Questi nuovi modelli sono solitamente indicati come **Chat LLM** per distinguerli dai più semplici e primitivi LLM.\n",
        "\n",
        "Questi nuovi modelli si contraddistinguono per alcune capacità funzionali:\n",
        "- sono **multimodali**, nel senso che possono ricevere in input e generare in output contenuti non limitati al solo testo. In particolare gestiscono nativamente immagini, video e suoni. (Nel nostro caso non abbiamo bisogno di utilizzare questa capacità).\n",
        "- prevedono l'impiego di \"**tool**\"\n",
        "- prevedono **messaggi** di input associati a **ruoli** specifici, ad esempio istruzioni di sistema, input utente, messaggi dell'assistente\n",
        "- hanno capacità di ragionamento avanzate e sono in grado di fornire istruzioni\n",
        "\n",
        "Ad oggi, questi modelli sono disponibili esclusivamente in modalità ***cloud computing***, accessibili mediante API REST.\n",
        "\n",
        "Per il nostro prototipo abbiamo scelto di utilizzare come modello: **Claude 3.5 v.2 di Anthropic**.\n",
        "\n",
        "Il software è tuttavia facilmente adattabile a qualunque altro modello di pari capacità.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FfnFiesTxTiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paradigma RAG - Retrieval Augmented Generation\n",
        "\n",
        "I **LLM** di ultima generazione mostrano capacità di comprensione e conoscenze in campi molto diversi tra loro, tanto da apparire quasi onniscienti.\n",
        "\n",
        "Il modello utilizzato in questo prototipo, ad esempio, è sicuramente in grado di rispondere in modo corretto a domande che riguardano argomenti molto specifici, relativamente ai quali esiste ampia documentazione di pubblico dominio, accessibile via Internet.\n",
        "\n",
        "I principali LLM di mercato incorporano queste conoscenze di pubblico dominio aggiornate alla data di addestramento del modello e sono oramai in grado di fornire indicazioni corrette con un elevato grado di probabilità.\n",
        "\n",
        "Questi LLM ovviamente non sono in grado di conoscere informazioni più recenti rispetto alla data del loro ultimo aggiornamento e informazioni specifiche di una azienda che non sono pubblicamente disponibili e accessibili.\n",
        "\n",
        "Pertanto, la realizzazione di un assistente basato su un modello pre-addestrato **SOA** (*State-Of-the-Art*), che sia in grado di offrire un servizio personalizzato per una particolare applicazione, richiede una programmazione specifica.\n",
        "\n",
        "Una prima soluzione potrebbe essere quella di effettuare un supplemento di addestramento (detto ***fine tuning***) del modello, per ottenere una versione \"personalizzata\" in grado di gestire la conoscenza specifica sul dominio applicativo.\n",
        "\n",
        "I servizi di *fine tuning* sono offerti da tutti i provider di LLM e la soluzione è tecnicamente praticabile. Tuttavia presenta due svantaggi:\n",
        "\n",
        "- Il **costo** del fine tuning e il successivo **costo** di accesso alla versione \"personalizzata\" del LLM sono molto **elevati**.\n",
        "- In caso di **aggiornamenti della documentazione** che fa parte della base di conoscenza specifica occorre procedere ad una **nuova**, costosa, **fase di fine tuning**.\n",
        "\n",
        "La seconda soluzione, in genere molto meno onerosa e più adatta al nostro contesto, sfrutta una caratteristica dei moderni **Chat LLM** che consiste nella possibilità di fornire assieme alla domanda dell'utente, una serie di **istruzioni** e **informazioni di contesto**.\n",
        "\n",
        "L'**input** ad un **Chat LLM** è un dato strutturato, solitamente in formato JSON, che contiene diversi campi detti ***messages***, ciascuno dei quali, oltre alla stringa di testo che è il contenuto vero e proprio del messaggio, contiene anche meta informazioni come ad esempio il **ruolo** dell'attore del messaggio (cioè a che titolo viene fornito il messaggio).\n",
        "\n",
        "Questa struttura è quella che propriamente viene chiamata **prompt**.\n",
        "\n",
        "Un prompt preparato per un moderno Chat LLM contiene solitamente diversi messaggi, come ad esempio:\n",
        "\n",
        "- Istruzioni operative che spiegano al LLM come comportarsi nell'elaborare la risposta, indicando eventuali vincoli o modalità di strutturazione della risposta. In genere questo tipo di messaggio ha un ruolo indicato come \"**system**\".\n",
        "- Contesto di riferimento, cioè un testo che contiene o esplicita informazioni aggiuntive che potrebbero non essere note al LLM oppure non essere quelle immediatamente utilizzate per elaborare la risposta. Un messaggio di questo tipo potrebbe avere un ruolo \"**system**\" ma alcuni LLM prevedono un ruolo specifico chiamato \"**assistant**\".\n",
        "- Messaggi che costituiscono la **storia precedente della conversazione**, ciascuno con i vari ruoli. Questo è importante per poter gestire correttamente i cosiddetti thread di conversazione. Le chiamate alle API degli LLM sono solitamente stateless.\n",
        "- Domanda dell'utente, in genere indicata con il ruolo \"**user**\".\n",
        "\n",
        "La struttura del prompt è specifica per ogni modello LLM.\n",
        "\n",
        "I **Chat LLM** sono specificamente addestrati per tenere in debita considerazione i messaggi e soprattutto i ruoli con i quali vengono forniti.\n",
        "\n",
        "Il paradigma **RAG** sfrutta questa caratteristica.\n",
        "In prima battuta potremmo pensare di inserire il testo di tutta la documentazione specifica come messaggio di contesto, in modo che il LLM risponda tenendo in considerazione queste informazioni.\n",
        "\n",
        "In realtà questo è possibile solo se tali informazioni, valutate in termini di parole (più esattamente *token*), hanno una dimensione compatibile con il LLM.\n",
        "Un LLM come quello che abbiamo utilizzato ha una \"finestra di contesto\" di circa 200.000 token. Si tratta di una buona capacità, ma sufficiente a gestire non più di due o tre manuali.\n",
        "Inoltre il costo del servizio di un Chat LLM è calcolato in base al numero di token forniti in input e generati in output.\n",
        "\n",
        "Sarebbe quindi più opportuno passare come \"contesto\" solo i documenti, o i paragrafi, pertinenti alla query dell'utente.\n",
        "\n",
        "È su questo principio che viene definito un tipico **processo RAG**:\n",
        "\n",
        "1. l'utente inserisce la sua **query**;\n",
        "2. il sistema utilizza la query per selezionare i **documenti (o paragrafi) pertinenti** dalla \"*base di conoscenza*\";\n",
        "3. il sistema prepara un **prompt** opportuno, composto da **istruzioni**, sequenza di **testi recuperati** al passo precedente e **query** dell'utente;\n",
        "4. il sistema **invoca** il LLM fornendo il **prompt** preparato al passo precedente;\n",
        "5. il LLM genera la sua **risposta** che sarà fortemente influenzata dal contesto e dalle istruzioni fornite.\n",
        "\n",
        "Con la tecnica **RAG** è possibile costruire ChatBot personalizzati che forniscono risposte corrette su specifici domini di conoscenza.\n"
      ],
      "metadata": {
        "id": "MZKwRyz_3Tlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "Nel paragrafo precedente è stato descritto un tipico processo **RAG**.\n",
        "\n",
        "Al **punto 2** è stato indicato che la **query** dell'utente viene utilizzata per **selezionare** dalla base di conoscenza i **documenti o i paragrafi pertinenti**.\n",
        "\n",
        "Si tratta dell'aspetto più critico in tutto il processo.\n",
        "\n",
        "Come nel nostro caso, molto spesso, la base di conoscenza è costituita da un insieme di documenti di varia origine e natura. **Come si individuano le parti di testo pertinenti?**\n",
        "\n",
        "Occorre avere una strutturazione della base di conoscenza che sia un **database** su cui possono essere effettuate **ricerche di tipo semantico**.\n",
        "\n",
        "È su questo aspetto che interviene la tecnologia degli **embeddings**.\n",
        "\n",
        "Si tratta di uno dei prodotti più importanti della ricerca sull'elaborazione del linguaggio naturale (**NLP**) che ha portato ai moderni LLM.\n",
        "\n",
        "La tecnica degli **embeddings** è una tecnica che permette di associare ad un testo, più o meno lungo, (frase, paragrafo o intero documento) un **vettore di numeri** di tipo **`float`** di lunghezza fissa.\n",
        "\n",
        "Gli algoritmi di **embeddings** sono a loro volta delle reti neurali addestrate su corpus di testo molto ampi.\n",
        "\n",
        "L'addestramento su enormi quantità di testi di questi algoritmi è una delle fasi propedeutiche allo sviluppo di un LLM.\n",
        "\n",
        "Un **modello di embeddings** è una **rete neurale**, solitamente basata su *transformers* che è stata addestrata a riconoscere le relazioni sintattiche e semantiche tra le parole (in realtà tra token) che compongono un testo.\n",
        "\n",
        "Il **vettore risultante** (*vettore di embeddings*) è quindi un insieme di numeri che rappresentano in qualche modo aspetti sintattici, ma soprattitto **semantici**, del testo di origine.\n",
        "\n",
        "La dimensione dei **vettori di embeddings** utilizzati nei moderni sistemi di IA oscilla tra 256 e 3072.\n",
        "\n",
        "Se consideriamo un **vettore di embeddings** come un punto in uno spazio n-dimensionale possiamo associare alle distanze geometriche tra punti (vettori) diversi la relazione semantica tra i corrispondenti testi di origine.\n",
        "\n",
        "Gli algoritmi di **embeddings** rendono possibile trattare le relazioni semantiche tra testi utilizzando formule matematico-geometriche.\n",
        "\n",
        "Gli **embeddings** sono la chiave per realizzare database con possibilità di ricera semantica.\n",
        "\n",
        "Nel nostro caso, abbiamo scelto di utilizzare come algoritmo di embeddings il recente modello **Text Embeddings 3 Large** di **OpenAI** che genera vettori di dimensione **3072**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WWcbinnq3ICw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database vettoriale\n",
        "\n",
        "Il **Database Vettoriale** è uno dei componenti chiave di una soluzione **RAG**.\n",
        "\n",
        "Si tratta di uno specifico DBMS ottimizzato per la memorizzazione di **vettori** e la relativa indicizzazione.\n",
        "\n",
        "In particolare, per implementare una soluzione **RAG** occorre un **database vettoriale** che sia indicizzato, cioè che associ i vettori memorizzati (che sono degli **embeddings**) con i relativi file di origine.\n",
        "\n",
        "Il **database vettoriale** o **vectorstore** costituisce la base di conoscenza su cui il processo **RAG** effettua la **ricerca semantica**.\n",
        "\n",
        "La sua costruzione e il suo popolamento deve tenere in considerazione le esigenze dimensionali sia dei LLM, sia degli algoritmi di embeddings.\n",
        "\n",
        "I **modelli di embeddings** hanno dei limiti alla lunghezza del testo che può essere fornito in input, così come i **LLM** hanno dei limiti sulla lunghezza complessiva del **prompt**.\n",
        "\n",
        "La costruzione del **vectorstore** avviene pertanto secondo il seguente processo:\n",
        "\n",
        "1. I documenti di origine vengono suddivisi in frammenti, chiamati **chunk**, di dimensioni adeguate (solitamente di alcune migliaia di parole o token);\n",
        "2. ciascun **chunk** viene fornito in input al modello di **embeddings** per ottenere il corrispondente vettore;\n",
        "3. ciascun **vettore** viene quindi aggiunto nel database vettoriale;\n",
        "4. il **database vettoriale** viene indicizzato correlando ogni **vettore** al suo **chunk** di testo. I **chunk** componenti un singolo documento sono correlati al documento di origine.\n",
        "\n",
        "Il **vectorsore** deve offrire **funzionalità di ricerca di tipo semantico**.\n",
        "\n",
        "Nel prototipo costruito in questo notebook abbiamo usato come DBMS vettoriale il prodotto *open source* **Chroma**.\n",
        "\n",
        "Si tratta di un prodotto molto leggero, serializzato su file system, specializzato per applicazioni di IA.\n",
        "\n",
        "In un contesto di produzione si può utilizzare un qualunque DBMS vettoriale di classe adeguata.\n",
        "\n",
        "L'offerta AWS è abbastanza ampia in questo settore."
      ],
      "metadata": {
        "id": "1SmOFH2Gc8by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architettura a catena o \"chain\"\n",
        "\n",
        "La struttura di massima descritta nel paragrafo dedicato al paradigma **RAG** rappresenta lo schema più semplice e controllabile per un assistente personalizzato.\n",
        "\n",
        "Usando questo tipo di architettura, l'applicazione viene realizzata programmando i singoli blocchi della sequenza.\n",
        "\n",
        "Il **flusso di esecuzione** segue un percorso lineare, detto *chain*.\n",
        "\n",
        "Ogni chiamata del ChatBot attiva un thread di esecuzione composto dalla sequenza lineare di operazioni: **`retrieval -> costruzione prompt -> invocazione del LLM`**.\n",
        "\n",
        "Questa soluzione fornisce allo sviluppatore il pieno controllo del flusso di elaborazione, ma in alcuni casi presenta una serie di svantaggi che vedremo più avanti, con degli esempi."
      ],
      "metadata": {
        "id": "TRpm1knS8VdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Struttura conversazionale e Chat History\n",
        "\n",
        "Lo schema di ChatBot descritto finora è di tipo ***stateless***.\n",
        "\n",
        "Ogni invocazione dell'assistente avvia un thread di elaborazione che non tiene conto di eventuali invocazioni precedenti effettuate dallo stesso utente.\n",
        "\n",
        "In altre parole, il ChatBot gestisce la query dell'utente in modo puntuale: botta e risposta, senza memoria dei messaggi scambiati in precedenza.\n",
        "\n",
        "Nella pratica, i chat bot a cui siamo abituati, ad esempio ChatGPT, permettono un'interazione in cui il contesto è significativo.\n",
        "\n",
        "Eventuali domande poste successivamente alla prima, possono implicitamente fare riferimento ai messaggi precedenti.\n",
        "\n",
        "Un frammento di conversazione reale, tratta da una applicazione che assiste gli utenti di un prodotto di e-procurement, potrebbe essere il seguente:\n",
        "\n",
        "- Utente: \"Il sistema gestisce le gare?\"\n",
        "- ChatBot: \"Si, il prodotto.... fornisce supporto, bla, bla, bla...\"\n",
        "- Utente: \"Come ne inserisco una?\"\n",
        "- ...\n",
        "\n",
        "È evidente che con una architettura di ChatBot stateless, la gestione della seconda domanda sarebbe imprecisa. La frase \"*Come ne inserisco una?*\" assume un senso preciso solo facendo riferimento ai messaggi precedentemente scambiati.\n",
        "\n",
        "Se si vuole realizzare un assistente in grado di gestire una **conversazione**, cioè un *thread* di domande e risposte, occorre gestire due nuovi elementi:\n",
        "\n",
        "1. la definizione di \"**sessioni**\" utente;\n",
        "2. il mantenimento della **storia** di tutti i messaggi scambiati durante la sessione.\n",
        "\n",
        "Le API offerte dai provider degli LLM sono di tipo *stateless*. Ad ogni nuova chiamata viene avviato sul cloud del provider un nuovo thread di esecuzione. Non viene offerto supporto per la gestione di sessioni. Ogni chiamata è una nuova sessione che si apre con l'invocazione del LLM e si chiude con la ricezione della risposta.\n",
        "\n",
        "La gestione delle sessioni e il mantenimento della cosiddetta **chat history** è a cura del software dell'assistente.\n",
        "\n"
      ],
      "metadata": {
        "id": "XViPe4M2-rUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool\n",
        "\n",
        "I LLM SOA prevedono l'esistenza di **tool**.\n",
        "\n",
        "Un **tool** è una funzione software in grado di elaboare informazioni, eseguire un compito, generare delle nuove informazioni.\n",
        "\n",
        "La disponibilità di una o più funzioni di questo tipo, è un parametro che può essere passato ad un LLM attraverso il **prompt**.\n",
        "\n",
        "Quello che riceve in input il LLM è un elenco di nomi di **tool** (sono identificatori definiti dallo sviluppatore) associati ad una descrizione in linguaggio naturale degli scopi del **tool**, della sua funzione e delle sue capacità.\n",
        "\n",
        "Se nel **prompt** viene fornita la **lista dei tool** il LLM può decidere di non rispondere immediatamente alla **query** dell'utente, ma di fornire una risposta (le risposte sono in genere strutture JSON) in cui è indicata l'esigenza di invocare uno o più **tool** con determinati parametri di input.\n",
        "\n",
        "È importante osservare che il LLM **non è in grado di eseguire o invocare direttamente le funzioni (tool)**, ma si limita a fornire come risposta una richiesta di invocazione dei **tool** appropriati.\n",
        "\n",
        "Il programma può essere strutturato per eseguire in modo automatico le chiamate alle relative funzioni, oppure può essere strutturato per demandare la decisione di eseguire una certa funzione all'utente. Quest'ultimo tipo di flusso viene definito ***man in the middle***.\n",
        "\n",
        "In ogni caso il controllo sull'esecuzione dei **tool** è demandato al software dell'assistente.\n",
        "\n",
        "Dopo aver chiamato le funzioni richieste, i risultati vengono passati al LLM con un nuovo **prompt**.\n",
        "\n",
        "Il LLM può chiedere ulteriori esecuzioni di **tool** finché la sua logica di elaborazione continua a fornire questo tipo di risultato, oppure fornire la risposta alla **query** e terminare il processo.\n",
        "\n",
        "Si tenga sempre a mente che la risposta, e quindi la logica di ragionamento, del LLM è influenzato anche dalle eventuali **istruzioni** fornite nel **prompt** a corredo della query utente e del contesto."
      ],
      "metadata": {
        "id": "rmDmzvflC33m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agenti e applicazioni agentiche\n",
        "\n",
        "La capacità di gestire i **tool**, da parte di un LLM, rende possibile strutturare l'applicazione in un modo diverso.\n",
        "\n",
        "Nella struttura a **chain** ogni blocco della catena può anche essere visto come una specifica ***funzione*** nel senso di modulo software realizzato con il linguaggio di programmazione.\n",
        "\n",
        "I singoli **blocchi**, cioè le singole **funzioni**, sono eseguite in un ordine prestabilito, o **secondo un flusso predeterminato dalla logica di controllo del programma** (tramite le classiche istruzioni `if..then..else` e `loop`).\n",
        "\n",
        "Se, al contrario, implementiamo ogni blocco come una effettiva funzione programmatica che rispetta la struttura di *input* ed *output* previste dalle API del LLM per la definizione dei **tool**, possiamo **demandare il controllo del flusso effettivo di elaborazione al LLM**.\n",
        "\n",
        "In altre parole, dopo aver fornito il **prompt** con la **query**, le **istruzioni**, il **contesto**, la **chat history** e la **lista dei tool**, sarà il LLM, tramite le sue risposte, a determinare le operazioni da eseguire, cioè le chiamate ai singoli **tool**, e quindi in ultima analisi a detrminare una certa sequenza di operazioni, per rispondere alla **query** posta dall'utente.\n",
        "\n",
        "Una struttura di questo tipo è chiamata **agente**.\n",
        "\n",
        "Più in generale, le applicazioni in cui il flusso di elaborazione è totalmente o parzialmente controllato tramite le risposte di un LLM sono definite **applicazioni agentiche**.\n",
        "\n",
        "Si tratta di sistemi molto versatili e potenti, ma meno controllabili rispetto alle **chain**."
      ],
      "metadata": {
        "id": "vtgBCEXEGgQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizzazione di un ChatBot personalizzato con tecnica RAG\n",
        "\n",
        "In questo Notebook sono esplorate e realizzate tutte le architetture e tutti i componenti descritti nella premessa.\n",
        "\n",
        "L'obiettivo è quello di mostrare soluzioni alternative e un toolkit di strumenti software per speriemntare, testare e validare le diverse architetture e i singoli componenti.\n",
        "\n",
        "Lo sviluppo del software è articolato in sezioni:\n",
        "\n",
        "1. Nella prima sezione viene mostrato come costruire un **vectorstore** e come popolare e indicizzare il **database vettoriale** effettuando il **chunking** e l'**embeddings** dei documenti che costituiscono la **base di conoscenza**. Vengono anche predisposte le istruzioni di utility per salvare e ripristinare il **database vettoriale** rendendolo persistente rispetto all'esecuzione del presente notebook.\n",
        "\n",
        "2. Nella seconda sezione viene costruito un **ChatBot RAG** di tipo **stateless** con architettura a **chain**.\n",
        "\n",
        "3. Nella terza sezione viene costruito un **ChatBot RAG** di tipo **conversazionale**, in grado di gestire **sessioni utente** e mantenere una **chat history**.\n",
        "\n",
        "4. Nella quarta sezione viene costruito un **ChatBot RAG** di tipo **agentico**."
      ],
      "metadata": {
        "id": "JAmfS4BhJwOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sviluppo del software\n",
        "\n",
        "### Linguaggio di programmazione\n",
        "\n",
        "Per lo sviluppo dell'assistente è stato utilizzato il linguaggio di programmazione **Python**.\n",
        "\n",
        "La scelta del **Python** è oggi obbligata da alcune importanti ragioni di opportunità:\n",
        "\n",
        "- Il Python è, allo stato dell'arte, il linguagio d'elezione dell'Intelligenza Artificiale. La disponibilità di strumenti e librerie in Python supera di gran lunga l'offerta di strumenti comparabili, disponibili in linguaggi come Java.\n",
        "- Il Python ha una sintassi più intuitiva, pulita e facilmente comprensibile anche a chi non conosce il linguaggio. Si presta pertanto ad essere uno strumento ideale per il trasferimento di know how.\n",
        "- Il Python è interpretato e si presta particolarmente bene ad essere utilizzato in modo interattivo e incrementale, rendemndo possibile strumenti come il presente notebook.\n",
        "- Tramite il Python è possibile abbattere i tempi di realizzazione di sistemi di IA come quello qui presentato.\n",
        "\n"
      ],
      "metadata": {
        "id": "cKO3HjUpNe36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Framework di astrazione\n",
        "\n",
        "Uno dei problemi nello sviluppo di applicazioni basate sulle tecnologie LLM più recenti è costituito dall'enorme fluidità del settore che è in continua e rapida evoluzione.\n",
        "\n",
        "Le prestazioni dell'applicazione potrebbero migliorare notevolmente nell'immediato futuro avvalendosi dei modelli di **embeddings** e **LLM** più recenti.\n",
        "\n",
        "Inoltre, i modelli scelti per l'implementazione, che oggi costituiscono lo stato dell'arte, potrebbero rapidamente risultare obsoleti.\n",
        "\n",
        "Questa situazione non costituisce un problema per chi si occupoa di ricerca e sviluppo, trattandosi di un contesto dove le variazioni di scenario e le innovazioni tecnologiche sono il contesto quotidiano, mentre può essere una importante criticità per un contesto di produzione.\n",
        "\n",
        "Ipotizzando come contesto di produzione un *deployment* su una infrastruttura cloud come **AWS** o **Microsoft Azure**, è facile constatare che l'offerta di servizi e modelli di IA, disponibili direttamente su queste piattaforme, è in costante evoluzione. La stessa documentazione ufficiale dei cloud provider fatica a stare dietro agli aggiornamenti.\n",
        "\n",
        "Ogni nuovo modello, sia di **embeddings** che **Chat LLM** porta con se le sue API e le sue specificità.\n",
        "\n",
        "I diversi modelli offerti dai provider leader di mercato sono simili nella logica astratta ma diversi nelle specifiche connesse alle strutture dati e ai messaggi in input e output.\n",
        "\n",
        "Una applicazione come quella qui presentata gestisce elementi importanti e complessi, quali **prompt**, **messaggi**, definizioni di **tool**, **query semantiche** e **chat history**.\n",
        "\n",
        "Una volta scelta e adottata una combinazioni di modelli, e relative API, il passaggio a servizi diversi comporta inevitabilmente un *refactoring* importante del codice sorgente.\n",
        "\n",
        "Per questo motivo è stata effettuata una scelta, molto comune nei contesti destinati allo sviluppo di applicazioni che andranno in produzione, di utilizzare una piattaforma di astrazione, specifica per la realizzazione di architetture a **chain** e **applicazioni agentiche** mediante **LLM SOA**.\n",
        "\n",
        "La scelta è inevitabilmente ricaduta sul prodotto, attualmente leader di mercato, denominato **LangChain**.\n",
        "\n",
        "#### LangChain\n",
        "\n",
        "**LangChain** offre allo sviluppatore un modello astratto per ciascuna delle entità che abbiamo descritto nella sezione generale.\n",
        "\n",
        "Attraverso API molto potenti, e ben documentate, è possibile realizzare con metodi dichiarativi **chain** di funzionalità tipiche delle applicazioni **RAG**, **vectorstore**, **chat history** e gestire **sessioni utente**.\n",
        "\n",
        "L'interfaccia delle API è **indipendente dai servizi effettivamente utilizzati**.\n",
        "\n",
        "**LangChain** offre l'integrazione delle sue API con la quasi totalità dei provider operanti sul mercato, ed è la soluzione raccomandata da tutti i leader di mercato, come **Amazon AWS**, **Microsoft Azure**, **OpenAI** e **Anthropic**.\n",
        "\n",
        "La migrazione verso soluzioni diverse è una operazione semplice e immediata, che richiede modifiche minime e puntuali al codice sorgente.\n",
        "\n",
        "La programmazione tramite **LangChain** viene svolta ad un livello di astrazione molto elevato e lo sviluppo di applicazioni robuste e solide dal punto di vista architetturale è estremamente veloce.\n"
      ],
      "metadata": {
        "id": "v1lyyPMMQCdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sviluppo del software\n",
        "\n",
        "## Installazione dei package Python necessari.\n",
        "\n",
        "L'istruzione contenuta nella cella seguente, è una istruzione Linux che installa tramite il Package Manager PIP, i moduli che useremo nel software:\n",
        "\n",
        "- **langchain** è il package principale delle librerie LangChain\n",
        "- **langchain_community** contiene le librerie definite dalla community di utenti di Lang Chain, da cui estrarremo alcune classi per il nostro software\n",
        "- **langchain-chroma** è il package che contiene l'interfaccia di integrazione verso il DB vettoriale **Chroma**\n",
        "- **pypdf** è una libreria molto usata in Python per la lettura dei file in formato PDF\n",
        "- **langchain-openai** è il package che contiene l'interfaccia di integrazione verso i servizi **OpenAI**\n",
        "- **langchain-anthropic** è il package che contiene l'interfaccia di integrazione verso i servizi **Anthropic**\n",
        "- **langgraph** è un package della suite LangChain che contiene le classi e le API per la definizione di applicazioni agentiche"
      ],
      "metadata": {
        "id": "ZbeAwButQA9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Rr3t9kRzpd"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_community langchain-chroma pypdf langchain-openai langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accesso ai provider AI\n",
        "\n",
        "Il nostro software utilizza sia i servizi di OpenAI che di Anthropic.\n",
        "\n",
        "Per entrambe le piattaforme, l'accesso alle relative API avviene mediante una chiave segreta, il cui valore può essere passato direttamente nei metodi di invocazione delle API oppure impostato in una variabile di ambiente.\n",
        "\n",
        "Inserire il valore della chiave segreta in modo esplicito nel codice non è una pratica consigliabile, per cui abbiamo adottato il secondo metodo.\n",
        "\n",
        "Nella cella di codice che segue, vengono impostati i valori delle variabili di ambiente a partire da due valori *secret* di Google Colab.\n",
        "\n",
        "I *secret* impostati su colab sono specifici dell'account Google dell'utente che esegue il notebook e non vengono memorizzati nel notebook."
      ],
      "metadata": {
        "id": "n_aRpPbqSFPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "ezlcviDFacBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sezione 1 - Realizzazione del Vectorstore\n",
        "\n",
        "**Il processo di realizzazione e gestione di un vectorstore per applicazioni RAG avviene off-line rispetto all'esecuzione dell'assistente.**\n",
        "\n",
        "La piattaforma DBMS utilizzata per creare il **database vettoriale** dell'applicazione, non influisce sulle prestazioni della nostra applicazione in termini di \"inferenza\".\n",
        "\n",
        "Tali prestazioni dipendono dal modello utilizzato per l'algoritmo di **embeddings**.\n",
        "\n",
        "Lo scopo del DB vettoriale è semplicemente quello di memorizzare i **vettori di embeddings** e indicizzarli in riferimento ai file di origine.\n",
        "\n",
        "In un contesto di produzione, useremo un sistema DBMS Vettoriale in grado di offrire prestazioni adeguate al carico transazionale previsto.\n",
        "\n",
        "Nel caso di questo notebook, possiamo usare il sistema **Chroma** perfettamente adeguato anche in relazione alla dimensione della Base di Conoscenza fornita.\n",
        "\n",
        "La cella seguente, contiene il codice di definizione di una **classe manager** che incapsula, in logica *object oriented*, le funzionalità di un **vectorstore** finalizzato alla nostra applicazione **RAG**.\n",
        "\n",
        "Per la realizzazione è stato utilizzato il **layer di integrazione** tra **LangChain** e **Chroma**.\n",
        "\n",
        "Il codice è facilmente interpretabile.\n",
        "\n",
        "Sono definiti i metodi per **inizializzare un nuovo vectorstore**, per **aggiungere**, **modificare** ed **eliminare** un singolo documento PDF e per **aggiungere tutti i file PDF presenti in una determinata directory**.\n",
        "\n",
        "I documenti caricati sono identificati da **chiavi univoche** generate automaticamente a partire dal nome del file. Un apposito file JSON mantiene la corrispondenza tra i file caricati e i rispettivi ID.\n",
        "\n",
        "Il **vectorstore** è stato dichiarato **persistente** attraverso la impostazione di una apposita **cartella** (directory) sul file system locale.\n",
        "\n",
        "La cella contiene esclusivamente la definizione della classe.\n",
        "\n",
        "Il codice di questa classe è spiegato più avanti."
      ],
      "metadata": {
        "id": "2ghd-wjfiolG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "class VectorStoreManager:\n",
        "    def __init__(self, persist_directory, embedding_model=\"text-embedding-3-large\"):\n",
        "        \"\"\"\n",
        "        Inizializza il VectorStoreManager.\n",
        "\n",
        "        :param persist_directory: Directory dove il vectorstore sarà persistente.\n",
        "        :param embedding_model: ID del modello di embeddings da utilizzare.\n",
        "        \"\"\"\n",
        "        self.persist_directory = persist_directory\n",
        "\n",
        "        # Inizializza l'embedding di OpenAI\n",
        "        self.embedding = OpenAIEmbeddings(\n",
        "            model=embedding_model\n",
        "        )\n",
        "\n",
        "        # Assicurati che la directory di persistenza esista\n",
        "        os.makedirs(self.persist_directory, exist_ok=True)\n",
        "\n",
        "        # Inizializza o carica il vectorstore\n",
        "        self.vectorstore = Chroma(\n",
        "            persist_directory=self.persist_directory,\n",
        "            embedding_function=self.embedding\n",
        "        )\n",
        "\n",
        "        # Carica la mappatura degli ID dei documenti\n",
        "        self.id_mapping_path = os.path.join(self.persist_directory, \"id_mapping.json\")\n",
        "        self.id_mapping = self._load_id_mapping()\n",
        "\n",
        "    def _load_id_mapping(self):\n",
        "        if os.path.exists(self.id_mapping_path):\n",
        "            with open(self.id_mapping_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def _save_id_mapping(self):\n",
        "        with open(self.id_mapping_path, 'w') as f:\n",
        "            json.dump(self.id_mapping, f, indent=4)\n",
        "\n",
        "    def _generate_unique_id(self, file_path):\n",
        "        return os.path.basename(file_path)\n",
        "\n",
        "    def load_documents_from_directory(self, directory_path, chunk_size=6000, chunk_overlap=200):\n",
        "        \"\"\"\n",
        "        Carica tutti i documenti PDF da una directory e li aggiunge al vectorstore.\n",
        "\n",
        "        :param directory_path: Percorso della directory contenente i PDF.\n",
        "        :param chunk_size: Dimensione dei chunk di testo.\n",
        "        :param chunk_overlap: Sovrapposizione tra i chunk.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(directory_path):\n",
        "            raise ValueError(f\"{directory_path} non è una directory valida.\")\n",
        "\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.lower().endswith('.pdf'):\n",
        "                file_path = os.path.join(directory_path, filename)\n",
        "                self.add_document(file_path, chunk_size, chunk_overlap)\n",
        "\n",
        "        print(f\"Tutti i documenti da {directory_path} sono stati caricati nel vectorstore.\")\n",
        "\n",
        "    def add_document(self, file_path, chunk_size=6000, chunk_overlap=200):\n",
        "        if not os.path.isfile(file_path):\n",
        "            print(f\"Il file {file_path} non esiste. Salto l'aggiunta.\")\n",
        "            return 0, None\n",
        "\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        try:\n",
        "            docs = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del file {file_path}: {e}\")\n",
        "            return 0, None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        splits = text_splitter.split_documents(docs)\n",
        "\n",
        "        id_documento = self._generate_unique_id(file_path)\n",
        "\n",
        "        for doc in splits:\n",
        "            doc.metadata['source_id'] = id_documento\n",
        "\n",
        "        self.vectorstore.add_documents(splits)\n",
        "        print(f\"Aggiunto documento {id_documento} con {len(splits)} chunk.\")\n",
        "\n",
        "        self.id_mapping[id_documento] = file_path\n",
        "        self._save_id_mapping()\n",
        "\n",
        "        return len(splits), id_documento\n",
        "\n",
        "    def remove_document(self, id_documento):\n",
        "        documents = self.vectorstore.get(where={\"source_id\": id_documento})\n",
        "\n",
        "        if not documents:\n",
        "            print(f\"Nessun documento trovato con ID: {id_documento}\")\n",
        "            return\n",
        "\n",
        "        ids_to_delete = documents['ids']\n",
        "        self.vectorstore.delete(ids=ids_to_delete)\n",
        "        print(f\"Rimosso documento con ID: {id_documento}\")\n",
        "\n",
        "        if id_documento in self.id_mapping:\n",
        "            del self.id_mapping[id_documento]\n",
        "            self._save_id_mapping()\n",
        "\n",
        "    def get_retriever(self):\n",
        "        return self.vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "fnLA_HkBS_kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe **`VectorStoreManager`** incorpora in un unico luogo tutte le funzionalità necessarie agli aspetti di **vettorializzazione** della **base di conoscenza** e recupero delle informazioni pertinenti sulla base di ricerche semantiche effettuate secondo il criterio della **similarity**.\n",
        "\n",
        "La classe è implementat attraverso le librerie specifiche offerte da **LangChain**, che permettono di semplificare lo sviluppo del codice.\n",
        "\n",
        "Gli oggetti appartenenti a questa classe comprendono un proprio database vettoriale Chroma, reso persistente su file system locale, i metodi per inserire, modificare e rimuovere documenti in formato PDF dal database vettoriale e un metodo (**`get_retriever`**) che consente di definire un oggetto della classe LangChain **`retriever`** che astrae le funzionalità di ricerca semantica del database vettoriale associato all'oggetto.\n",
        "\n",
        "I parametri passati al costruttore della classe sono:\n",
        "- il ***path*** della directory (cartella) locale in cui gestire la persistenza\n",
        "- il modello di **embeddings** da utilizzare per la vettorializzazione dei documenti. Questo parametro ha come default il modello **text-embeddings-3-large**.\n",
        "\n",
        "Il costruttore inizializza il client Bedrock configurandone l'accesso al modello di **embeddings** passato come parametro.\n",
        "L'operazione viene gestita dalle API di LangChain di integrazione verso i servizi OpenAI.\n",
        "La struttura dell'API **`OpenAIEmbeddings`** è analoga alla struttura di tutte le API di integrazione verso qualunque *provider* di modelli IA integrato da **LangChain**. Questo rende immediato e rapido un eventuale, futuro, porting del software verso altri provider.\n",
        "\n",
        "Successivamente viene **inizializzato** il **database vettoriale** attraverso due sole istruzioni:\n",
        "\n",
        "- `os.makedirs(self.persist_directory, exist_ok=True)` è un'istruzione di libreria standard Python che verifica l'esistenza della directory (cartella) passata come parametro. Se la cartella non esiste la crea nel path indicato.\n",
        "- ```\n",
        "  self.vectorstore = Chroma(\n",
        "  persist_directory=self.persist_directory,\n",
        "  embedding_function=self.embedding)\n",
        "  ```\n",
        "  Assegna all'attributo **vectorsore** un oggetto LangChain di tipo *vectorstore*, implementato con Chroma, e reso persistente attraverso la cartella di cui al punto precedente. Se la cartella era presesitente, **vectorstore** punta al database esistente nella cartella, se la cartella è stata appena creata, viene creato al suo interno un database vuoto.\n",
        "\n",
        "Si noti che un oggetto **vectorstore** LangChain è un wrapper che incorpora un normale **database vettoriale** abbinato a funzionalità di **embeddings** offerte dal modello specificato.\n",
        "\n",
        "Qualunque sia la complessità del DBMS utilizzato, le API di integrazione di LangChain consentono il setup del **vectorstore** secondo questo tipo di schema di codice, astratto e semplice.\n",
        "\n",
        "L'ultima parte di codice del costruttore, riguarda il completamento della struttura di persistenza, con la gestione di una mappa di identificatori di documento.\n",
        "\n",
        "Questa parte non è una funzione particolare di LangChain ma è un semplice metodo di indicizzazione che abbiamo deciso di implementare per tenere traccia dei documenti vettorializzati, come sarà chiaro fra poco.\n",
        "\n",
        "Per comprendere come viene popolato il **vectorstore** possiamo fare riferimento ad uno dei metodi per l'inserimento di un nuovo documento.\n",
        "\n",
        "Prendiamo come esempio il più semplice: **add_document**\n",
        "\n",
        "Questo metodo riceve come parametro il path di un documento PDF, che avremo preventivamente salvato sul nostro file system e lo inserisce nel **vectorstore** dopo aver svolto alcuni importanti passaggi:\n",
        "\n",
        "1. Per prima cosa il documento PDF viene processato da una funzione \"**loader**\", che fa parte delle API di **LangChain**, che sfrutta la libreria standard **pypdf** per trasformare il file PDF in una lista di stringhe di testo, ciascuna corrispondente ad una pagina del documento PDF.\n",
        "2. Successivamente ciascuna pagina, oramai in formato txt, viene eventualmente ridotta in frammenti (chunk). La dimensione di questi frammenti è controllata dal parametro **chunk_size**. Questa frammentazione dei testi, viene effettuata in modo da garantire una completezza sintattica e semantica di ciascun frammento. Il parametro **chunk_overlap** determina quanti caratteri o token di sovrapposizione si desiderano tra la fine di un frammento e l'inizio del successivo. Nel nostro caso il default impostato per **chunk_size** è di 6.000 token. Poiché il loader PDF di LangChain ha già suddiviso il documento di partenza in pagine, difficilmente le pagine di un manuale o di una circolare saranno superiori a 6.000 token, per cui è probabile che le pagine non siano ulteriormente suddivise. Il parametro 6.000 è un limite euristico per casi d'uso come il nostro che minimizza i costi delle invocazioni ai modelli LLM senza penalizzare le prestazioni semantiche. L'eventuale ulteriore frammentazione sarebbe comunque demandata alla classe LangChain **RecursiveCharacterTextSplitter** che implementa un algoritmo di frammentazione semantica ben costruito.\n",
        "3. I singoli frammenti di testo (chunk), che si suppongono sintatticamente e semanticamente completi, vengono quindi sottoposti ad **embeddings**, utilizzando il modello impostato, e aggiunti al **vectorstore**.\n",
        "\n",
        "Si noti che a questo punto, il vettori contenuti nel **vectorstore** fanno riferimento ai singoli **chunk** e non al documento originale. Un tipico manuale di 100 pagine verrebbe probabilmente ridotto in 100 chunk diversi, uno per ciascuna pagina e nel database vettoriale troveremo 100 vettori, ciascuno dei quali indicizzato e riferito al corrispondente testo.\n",
        "\n",
        "Nel momento in cui andremo ad inserire ulteriori documenti PDF non sapremmo più a quale documento di origine sono riferiti i singoli vettori.\n",
        "\n",
        "Questo non è assolutamente un problema per lo schema **RAG** in cui quello che conta è lo specifico frammento di informazione recuperato.\n",
        "\n",
        "Diventa un problema se vogliamo manutenere la base di conoscenza. Potremmo ad esempio voler eliminare o sostituire un manuale e in questo caso dovremmo effettuare una eliminazione di tutti i vettori associati ad uno specifico documento.\n",
        "\n",
        "Questo legame può essere gestito con i meta campi del database vettoriale.\n",
        "\n",
        "Nel nostro caso abbiamo utilizzato un meccanismo piuttosto banale. A partire dal nome del file del documento, generiamo una stringa che costituisce un **ID univoco** per il particolare docuemnto.\n",
        "\n",
        "Manteniamo traccia del mapping tra gli identificatori e i path d'origine dei documenti in un file in formato JSON che abbiamo aggiunto nella cartella di persistenza del nostro DB.\n",
        "\n",
        "Quando aggiungiamo un chunk, registriamo nelle meta informazioni l'ID del documento di provenienza.\n",
        "\n",
        "In questo modo saremo sempre in grado di eliminare dal DB tutti i chunk di un dato documento.\n",
        "\n",
        "I metodi di aggiornamento del **vectorstore** che abbiamo implementato funzionano su questo principio e dovrebbero essere facilmente comprensibili.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V7pmOPD6TmXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formato dei documenti sorgenti della Base di Conoscenza\n",
        "\n",
        "La **base di conoscenza** fornita da Net4Market è quella tipica di ogni organizzazione.\n",
        "\n",
        "Le informazioni sono memorizzate su file di vario formato: txt, PDF, Microsoft Office (.doc, .xls, .ppt), Open Office (.odt), ...\n",
        "\n",
        "Le operazioni di ***text splitting*** ed **embeddings** rappresentano sempre un aspetto critico nella realizzazione dei sistemi RAG.\n",
        "\n",
        "In particolare, è importante garantire che i singoli **chunk** mantengano  un significato semantico completo, cioè non abbiano un inizio o una fine che interrompano la sintassi di una proposizione, rendendo incomprensibile la frase.\n",
        "\n",
        "Un altro aspetto da considerare è che i testi dei **chunk** siano semanticamente correlati al contesto di origine.\n",
        "\n",
        "Ad esempio estraendo a caso una pagina dal manuale di un prodotto, dove viene mostrato il modo in cui effettuare il log-in ad una applicazione, potrebbe non essere più evidente a quale prodotto specifico si riferisca l'istruzione.\n",
        "\n",
        "Ovviamente se nella pagina è presente il titolo del prodotto o sono presenti header e footer in cui è citato il prodotto, il capitolo o il manuale di origine, questi frammenti di informazione possono orientare il LLM a comprendere meglio il contesto.\n",
        "\n",
        "Per questo motivo sono stati sviluppati diversi algoritmi di **text splitting** che agiscono creando **chunk** di buona qualità, sfruttando anche le meta informazioni presenti nei formati di file come il PDF o il formato Office.\n",
        "\n",
        "LangChain offre specifiche funzioni **loader** ottimizzate sui formati commerciali più diffusi.\n",
        "\n",
        "Tuttavia, per evitare di complicare il processo di popolamento del **vectorstore** una buona pratica, che si è affermata nel settore, è quella di preprocessare tutti i file, convertendoli in formato PDF.\n",
        "\n",
        "Il PDF è un formato sempre disponibile su tutti gli strumenti Office ed è un formato che preserva o, in molti casi, genera le meta informazioni, utilissime agli algoritmi di **text splitting**\n",
        "\n",
        "Per questo motivo, nel nostro prototipo abbiamo utilizzato esclusivamente il formato PDF."
      ],
      "metadata": {
        "id": "MAtSEROnuRuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inizializzazione del vectorstore"
      ],
      "metadata": {
        "id": "h3Z7s2c7miML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una volta che abbiamo definito la classe **`VectorStoreManager`**, possiamo creare fisicamente il nostro **vectorstore**.\n",
        "\n",
        "La cella che segue, imposta il path della directory in cui memorizzare fisicamente il DB vettoriale e inizializza il **vectorstore** creando l'oggetto **manager** che rappresenterà il nostro **vectorstore**.\n",
        "\n",
        "I casi d'uso in cui possiamo eseguire la cella di codice sono due:\n",
        "\n",
        "1. Vogliamo creare un nuovo *vectorstore* vuoto.\n",
        "2. Abbiamo già un DB vettoriale dentro una cartella sul file system e vogliamo semplicemente inizializzare l'oggetto **manager** affinché *punti* a tale DB.\n",
        "\n",
        "Nel primo caso, sarà sufficiente impostare il path desiderato come valore della variabile ***persist_dir*** e lanciare l'esecuzione del codice.\n",
        "Verrà creata una cartella nel path indicato e all'interno saranno presenti i file della struttura DB, ma saranno privi di dati.\n",
        "\n",
        "Nel secondo caso, sarà sufficiente impostare il valore della variabile **`persist_dir`** con il path della cartella in cui è presente il DB esistente e lanciare l'esecuzione del codice.\n",
        "\n",
        "Più avanti mostreremo come usare il comando **zip** per salvare sul disco del proprio PC la cartella contenente il DB generata sull'area file system di questo notebook, e come ripristinare dal file sul proprio PC una cartella con il DB nell'area file system di questo notebook."
      ],
      "metadata": {
        "id": "lJ8TT5BRjuWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specifica la directory di persistenza\n",
        "persist_dir = \"chroma_persist\"\n",
        "\n",
        "# Inizializza il manager\n",
        "manager = VectorStoreManager(persist_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "acYxSgAhoY2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gestione del vestorstore\n",
        "\n",
        "Vediamo ora alcuni esempi di codice che permettono di inserire, eliminare o aggiornare documenti nel **vectorstore** che abbiamo creato.\n",
        "\n",
        "Il caso più semplice è quello mostrato dalla cella di codice che segue.\n",
        "\n",
        "#### Aggiunta di un documento singolo\n",
        "\n",
        "Prendiamo un singolo documento PDF, carichiamolo sul file system di questo notebook, prendiamo nota del path e lanciamo l'esecuzione della cella, dopo aver aggiornato correttamente il path del documento.\n",
        "\n",
        "Il file sarà frammentato in **chunk**, sottoposto ad **embeddings** e aggiunto al **vectorstore**."
      ],
      "metadata": {
        "id": "qW3yFNU5mwvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggiunge un nuovo documento\n",
        "nuovo_documento_path = \"/content/Z9RG_(It)05.pdf\"  # Sostituisci con il percorso reale\n",
        "num_chunks_aggiunti, id_nuovo_documento = manager.add_document(nuovo_documento_path)\n",
        "if id_nuovo_documento:\n",
        "    print(f\"Numero di chunk aggiunti: {num_chunks_aggiunti}\")\n",
        "    print(f\"ID del nuovo documento: {id_nuovo_documento}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W54fpOKXiWrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eliminazione di un documento dal vectorstore\n",
        "\n",
        "Se vogliamo eliminare dal **vectorstore** un documento precedentemente caricato, dobbiamo conoscere il suo ID univoco.\n",
        "\n",
        "Nel nostro banale algoritmo di generazione degli ID, l'ID è semplicemente la stringa con il nome del file, senza l'intero path.\n",
        "\n",
        "In ogni caso, la lettura del file **id_mapping.json** presente nella cartella principale del DB mostra gli ID dei documenti presenti nel DB.\n",
        "\n",
        "Sarà sufficiente modificare il codice seguente con l'ID del documento che vogliamo rimuovere e lanciare l'esecuzione della cella di codice."
      ],
      "metadata": {
        "id": "QcPAGZMJoCBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manager.remove_document(\"Z9UMEUR_(It)03.pdf\")"
      ],
      "metadata": {
        "id": "Jcx9xoJFpRsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inserimento batch di più docuemnti\n",
        "\n",
        "Per un popolamento iniziale del **vectorstore** abbiamo predisposto un metodo che carica tutti i documenti presenti in una cartella sul file system.\n",
        "\n",
        "Creiamo una cartella sul file system di questo notebook con il nome che riteniamo più opportuno, copiamo in questa cartella i documenti PDF che vogliamo aggiungere al **vectorstore** modifichiamo il codice seguente con il path della cartella e lanciamo l'esecuzione della cella di codice.\n",
        "\n",
        "I file presenti nella cartella saranno caricati uno ad uno nel DB con la tecnica già descritta."
      ],
      "metadata": {
        "id": "-FzWyoUopS3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents_directory = \"/content/KnowledgeBase\"  # Sostituisci con il percorso reale\n",
        "manager.load_documents_from_directory(documents_directory)"
      ],
      "metadata": {
        "id": "h3KHXswNqCDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggiornamento di un documento\n",
        "\n",
        "Per aggiornare un documento presente nel DB sarà sufficiente eliminarlo e caricare la nuova versione.\n",
        "\n",
        "Nella definizione della classe **`VectorStoreManager`** abbiamo anche incluso un metodo che fa esattamente questo."
      ],
      "metadata": {
        "id": "WkqUk8OTqKkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backup e ripristino del vectorstore\n",
        "\n",
        "Se vogliamo salvare sul nostro PC il **vectorstore** possiamo eseguire l'istruzione Linux seguente e poi scaricare dall'area file system di questo notebook il file .zip contenente tutta la cartella di persistenza del DB."
      ],
      "metadata": {
        "id": "ikQrkJY0qsHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r chroma_persist.zip /content/chroma_persist/\n"
      ],
      "metadata": {
        "id": "JNvx31WlkYHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per ripristinare nell'area file system di questo notebook un DB a partire dal file .zip salvato sul nostro PC, possiamo caricare tale file nel file system del notebook ed eseguire il seguente comando Linux."
      ],
      "metadata": {
        "id": "7R6WpxFGrGHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chroma_persist.zip -d ."
      ],
      "metadata": {
        "id": "aF2Pep6VB-Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recupero del retriever\n",
        "\n",
        "In LangChain un **retriever** è un oggetto specializzato nel recupero di informazioni mediante **query semantiche**.\n",
        "\n",
        "Il **retriever** è un oggetto **runnable** (spiegheremo fra poco cosa vuol dire) che può essere facilmente assemblato in una **chain** o trasformato in un **tool**.\n",
        "\n",
        "Tutti gli oggetti di tipo **vectorstore** in LangChain offrono un metodo chiamato as_retriever, che permette di generare un oggetto **retriever** associato al **vectorstore**.\n",
        "\n",
        "Nel nostro caso, poiché abbiamo definito una classe wrapper per il **vectorstore** abbiamo anche incluso un metodo per recuperare il relativo **retriever**.\n",
        "\n",
        "Il codice che segue, recupera il **retriever** dall'oggetto **manager** creato in precedenza e lo assegna all'oggetto **`retriever`**."
      ],
      "metadata": {
        "id": "LOqTJyv9y0Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottenere il retriever\n",
        "retriever = manager.get_retriever()"
      ],
      "metadata": {
        "id": "aVOQ4sly1wa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una volta ottenuto il **retriever** possiamo sperimentare il suo funzionamento chiamando il metodo **`invoke`**, offerto da tutti i **runnable** LangChain, passando una **query** qualsiasi.\n",
        "\n",
        "Eseguendo la cella che segue, eventualmente modificango la stringa di query, possiamo vedere il testo grezzo estratto dalla ricerca semantica sul **vectorstore**.\n",
        "\n",
        "Quando invochiamo il **retriever** quello che accade dietro le quinte è il processo seguente:\n",
        "\n",
        "1. Viene invocato il modello di embeddings impostato all'atto della creazione del vectorsore per calcolare il **vettore di embeddings** della **query**. Nel nostro caso viene invocato il servizio AWS Bedrock passando la query al modello **Amazon Titan Embeddings text V2**;\n",
        "2. Il **vettore di embeddings** della **query** viene usato per effettuare la ricerca, e il recupero, dei **chunk** più affini, secondo un criterio algoritmico chiamato **similarity** che usa una formula matematica detta del **minimo seno**.\n",
        "3. I **chunk** recuperati vengono confezionati in un tipo di dato adatto ad essere gestito da una **chain** LangChain.\n",
        "\n",
        "Il tutto avviene senza che lo sviluppatore debba preoccuparsi dei dettagli."
      ],
      "metadata": {
        "id": "Xa8dHt7N0XAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"esiste l'esposizione spot?\") # Modificare la query con quella desiderata"
      ],
      "metadata": {
        "id": "-wX9rcaMfqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sezione 2 - Costruzione di un ChatBot RAG stateless mediante chain"
      ],
      "metadata": {
        "id": "6CZE0OwpGaOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa sezione viene mostrato come realizzare lo schema base di un processo **RAG** utilizzando il framework **LangChain**.\n",
        "\n",
        "Il **ChatBot** che viene implementato in questa sezione è di tipo **stateless**, cioè non è influenzato da eventuali precedenti chiamate da parte dell'utente.\n",
        "\n",
        "In uno scenario di produzione, ogni invocazione del servizio attiverebbe un nuovo *thread* indipendente che nasce con la chiamata e termina con la restituzione della risposta.\n",
        "\n",
        "Per motivi puramente didattici, abbiamo realizzato questo ChatBot assemblando i diversi componenti in una ***chain*** definita ad hoc.\n",
        "\n",
        "Come vedremo successivamente, per schemi di processo consolidati, **LangChain** offre funzioni **built-in** che semplificano notevolmente lo sviluppo del software.\n",
        "\n",
        "Nella cella seguente sono richiamate le classi e i metodi che useremo per costruire il ChatBot.\n",
        "\n",
        "In particolare:\n",
        "\n",
        "- **PromptTemplete** è una classe di LangChain, molto potente, che permette di costruire facilmente e dinamicamente i prompt.\n",
        "- **StrOutputParser** è una classe LangChain di utilità che permette di estrarre dagli oggetti restituiti come risposta dai LLM il solo campo contenente la stringa di testo.\n",
        "- **RunnablePassthrough** è un metodo di utilità di LangChain che consente di trasferire in output, senza alcuna modifica, un oggetto ricevuto in input, in un qualunque elemento di una chain."
      ],
      "metadata": {
        "id": "sMaL9qEeQTJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "9FNBgohhfcmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scelta del modello Chat LLM\n",
        "\n",
        "L'istruzione che segue, crea un oggetto, chiamato **llm** che rappresenta il client di accesso al modello LLM che abbiamo deciso di utilizzare.\n",
        "\n",
        "Prima di creare il modello viene importata la classe **ChatBedrock** che gestisce l'integrazione di **LangChain** verso i cosiddetti **modelli fondazione di base** offerti da **Amazon** tramite il servizio **AWS Bedrock**."
      ],
      "metadata": {
        "id": "-EKfX5EvTsK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n"
      ],
      "metadata": {
        "id": "6PQR-aISi_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"esiste l'esposizione spot?\")"
      ],
      "metadata": {
        "id": "PsuIyDwSpJFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Costruzione della chain e dei componenti elementari\n",
        "\n",
        "Prima di costruire la **chain** vera e propria, abbiamo bisogno di alcuni componenti.\n",
        "\n",
        "La cella che segue dichiara una funzione di utilità, che servirà a compattare i **chunk** di testo, recuperati dal **retriever**, in una unica stringa di testo.\n",
        "\n",
        "Infatti, provando ad invocare il **retriever**, come mostrato nella precedente sezione, l'oggetto di ritorno è una lista di oggetti strutturati (si tratta di oggetti della classe LangChain *document*), che nel campo **.content** contengono la stringa di testo che costituisce il contenuto che ci interessa.\n",
        "\n",
        "La funzione **format_docs** riceve in input una *lista* di oggetti di tipo *LangChain document* e, usando il metodo standard delle stringhe Python *.join* li aggrega in una unica stringa, in cui tra un blocco di testo e l'altro sono inseriti due caratteri *newline*."
      ],
      "metadata": {
        "id": "W3uPTYigVYGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "1UD7XFEmGrHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella di codice che segue mostra l'effetto della funzione appena definita"
      ],
      "metadata": {
        "id": "FLyooWW8H0MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = retriever.invoke(\"esiste l'esposizione spot?\")\n",
        "print('result =',result)\n",
        "print(\"context: \",format_docs(result))"
      ],
      "metadata": {
        "id": "q3Kn42rKZk8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice seguente crea l'oggetto *runnable*, chiamato **prompt**, di tipo **PromptTemplate**.\n",
        "\n",
        "I *prompt template* sono un ulteriore esempio dei potenti strumenti offerti da LangChain.\n",
        "\n",
        "La costruzione accurata dei prompt da inviare ai LLM è un elemento fondamentale per garantire l'efficacia delle applicazioni.\n",
        "\n",
        "Il **prompt engineering** è oggi una vera e propria disciplina.\n",
        "\n",
        "La stringa **template** contiene la struttura base del prompt, le istruzioni fisse in italiano, che dovranno sempre essere inviate al modello e due *placeholder* chiamati **question** e **context**.\n",
        "\n",
        "Il costruttore **PromptTemplete** genera l'oggetto **prompt** specificando che  sono previste due variabili di input, chiamate **context** e **question**.\n",
        "\n",
        "Il **prompt effettivo** (prompt value) che sarà inviato al modello LLM in questo caso è molto semplice. Le istruzioni di comportamento, la domanda dell'utente e il contesto sono raggruppati in un unico messaggio che sarà inviato con il **ruolo** di **user**.\n",
        "\n",
        "Nel paragrafo successivo vedremo come strutturare un prompt più efficace utilizzando messaggi diversificati per ruolo."
      ],
      "metadata": {
        "id": "AeNymn1XXTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Sei un assistente che aiuta l'utente a trovare soluzioni alle procedure di uso del prodotto.\n",
        "Usa come contesto i contenuti recuperati dal manuale utente. Per la risposta usa anche le tue competenze generali,\n",
        "ma se il contesto non contiene elementi pertinenti consiglia di rivolgersi ad un assistente umano.\n",
        "\n",
        "Domanda: {question}\n",
        "Contesto: {context}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template\n",
        ")\n"
      ],
      "metadata": {
        "id": "QEwq5TtmGOi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'istruzione seguente genera l'oggetto **rag_chain** che è il nostro **ChatBot RAG**\n",
        "\n",
        "È giunto il momento di fornire alcuni dettagli sui concetti di **runnable** e **chain**, che costituiscono il cuore e l'essenza del framework **LangChain**.\n",
        "\n",
        "In LangChain un **runnable** è un oggetto assimilabile ad una **funzione eseguibile**, ma in realtà molto più potente.\n",
        "\n",
        "La classe **runnable** prevede una serie di metodi per avviare l'esecuzione della funzione in scenari e contesti diversi.\n",
        "\n",
        "Il metodo **.invoke** ad esempio è quello utilizzato nelle chiamate automatiche che avvengono durante l'esecuzione di una **chain**, ma in realtà è possibile lanciare l'esecuzione sincrona di un *runnable* anche attraverso i metodi **stream** e **batch**. Inoltre, esiste anche la possibilità di lanciare i *runnable* con modalità asincrone, usando i metodi **ainvoke**, **abatch** e **astream**.\n",
        "\n",
        "Ciascun metodo prevede specifici schemi per l'input e l'output.\n",
        "\n",
        "La nostra **rag_chain** è costruita attraverso il formalismo LCEL di LangChain, che sfrutta l'operatore '|' (pipe).\n",
        "\n",
        "La *chain* è a sua volta un oggetto **runnable** per cui sarà lanciato con il metodo **invoke**.\n",
        "\n",
        "Come parametro, passeremo una stringa, che rappresenta la query dell'utente.\n",
        "\n",
        "Questa stringa viene fornita in input al primo elemento della chain che è la definizione esplicita di un dizionario Python:\n",
        "\n",
        " `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}`\n",
        "\n",
        " Il dizionario contiene due chiavi, **context** e **question**, che sono proprio le due variabili attese in input dal prompt template **prompt** che abbiamo definito prima.\n",
        "\n",
        " Alla chiave **context** viene associato l'output di una mini chain costituita da due elementi: **`retriever | format_docs`**.\n",
        "\n",
        " Il primo elemento è l'oggetto **retriever** che abbiamo costruito nella sezione deicata al **vectorstore** e il secondo elemento è la funzione di utilità che abbiamo costruito sopra.\n",
        "\n",
        " Il valore della stringa che contiene la **query** dell'utente, viene quindi ricevuta in input dal **retriever** che effettua la sua ricerca semantica restituendo la lista di oggetti *LangChain document* corrispondenti ai chunk di testo recuperati. Questa lista viene passata in input alla funzione **format_docs** che restituisce una singola stringa che viene associata alla chiave **context**.\n",
        "\n",
        " Alla chiave **question** viene associato l'output della funzione **RunnablePassThrough** che è esattamente il valore fornito in input, cioè il testo della **query** dell'utente.\n",
        "\n",
        " Questo dizionario è l'input al secondo elemento della **rag_chain** che è il prompt template **prompt**, che riceve le sue due variabili e fornisce in output un **prompt value** che contiene: istruzioni, contesto e query dell'utente.\n",
        "\n",
        " Questo prompt viene passato in input al successivo elemento della chain, che è l'oggetto **llm**.\n",
        "\n",
        " A questo punto sarà invocato il LLM e la risposta ricevuta sarà inviata all'ultimo elemento della catena, che è la funzione **StrOutputParser** che restituirà semplicemente il messaggio di risposta.\n"
      ],
      "metadata": {
        "id": "fGHUroxnZmGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SistemaRAG = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "XHYUprU_Gv9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **chain** che abbiamo appena definito è un **runnable** che possiamo eseguire come mostrato nel codice seguente.\n",
        "\n",
        "La stringa in output è in genere costituita da testo formattato mediante il linguaggio **MarkDown**, per cui abbiamo utilizzato delle funzioni di libreria, specifiche per i notebbok Jupyter, in modo da visualizzare il testo in modo formattato."
      ],
      "metadata": {
        "id": "FMqH8r9wfIYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"posso impostare l'esposizione spot?\"\n",
        "answer = SistemaRAG.invoke(query)\n",
        "\n",
        "# stampo la risposta con il corretto formato\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import output\n",
        "\n",
        "display(Markdown(answer))"
      ],
      "metadata": {
        "id": "uRrcLtYXGr_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "\n",
        "Come risulta evidente dall'esecuzione del nostro ChatBot, la risposta da parte di un LLM non è immediata.\n",
        "\n",
        "Per gestire il problema *psicologico* di una attesa che può apparire eccessiva all'utente esistono diverse tecniche.\n",
        "\n",
        "Una possibile tecnica è quella di mostrare in output le singole parole man mano che vengono generate dal LLM.\n",
        "\n",
        "La maggior parte dei LLM, infatti, fornisce la risposta in una modalità chiamata **streaming**.\n",
        "\n",
        "Nella cella seguente viene mostrato come eseguire la **chain** in questa modalità."
      ],
      "metadata": {
        "id": "fC8oyfyLgDJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in SistemaRAG.stream(\"Come imposto l'esposizione matrix?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "h0Er88aiG9FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semplificazione del codice con le funzioni built-in di LangChain\n",
        "\n",
        "Nel codice precedente, per motivi didattici, abbiamo mostrato come realizzare una **chain** usando il formalismo ***LCEL***.\n",
        "\n",
        "Questo metodo è utile quando si devono costruire workflow molto complessi per applicazioni particolari.\n",
        "\n",
        "Quando si implementano sistemi per casi d'uso comuni, sui quali esistono delle **best practices** consolidate, molto probabilmente il framework LangChain offre delle funzioni predefinite **built-in** che costruiscono la **chain** nel rispetto di queste **best practices**.\n",
        "\n",
        "Essendo il ChatBot RAG un tipico esempio di problema consolidato, possiamo implementare lo stesso ChatBot stateless in modo molto più immediato nel modo seguente."
      ],
      "metadata": {
        "id": "zVrE4SLD5af8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"Sei un assistente che aiuta l'utente a trovare soluzioni alle procedure di uso del prodotto. \"\n",
        "    \"Usa come contesto i contenuti recuperati dal manuale utente. \"\n",
        "    \"Per la risposta usa anche le tue competenze generali,\"\n",
        "    \"ma se il contesto non contiene elementi pertinenti consiglia di rivolgersi ad un assistente umano.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
      ],
      "metadata": {
        "id": "MyQjy5yw7BRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il metodo **create_stuff_documents_chain** costruisce automaticamente una chain in cui un insieme di elementi di testo (*documents* nella terminologia LangChain) viene passato così come'è (*stuff* ha il significato di inserimento) ad un LLM. I parametri di input sono il LLM e un prompt template che deve avere almeno una variabile denominata \"context\".\n",
        "La chain prodotta da questo metodo si aspetta in input un dizionario che abbia obbligatoriamente una chiave chiamata \"context\", oltre ad eventuali chiavi definite nel prompt template.\n",
        "L'output della chain generata dipende dal parametro OutParser che eventualmente viene passato. Non avendo passato questo parametro, il default sarà il parser *StrOutputParser* per cui l'output sarà una semplice stringa di testo.\n",
        "\n",
        "Questo metodo, come si vede, costruisce gran parte della chain che abbiamo definito in modo esplicito precedentemente.\n",
        "\n",
        "Il metodo **create_retrieval_chain** aggiungfe ad una chain un passo di **retrieval**. L'output restituito è un dizionario in cui la risposta è associata ad una chiave chiamata \"**answer**\".\n",
        "\n",
        "Per cui passando il nostro oggetto **retriever** e la chain generata dal metodo precedente abbiamo costruito la nostra **rag_chain**.\n",
        "\n",
        "Anche il metodo utilizzato per costruire il **prompt template** è leggermente diverso.\n",
        "\n",
        "In questo caso il template è strutturato come sequenza di messaggi e non come unico messaggio. Il **prompt value** sarà quindi composto da diversi messaggi che saranno inviati al **LLM** con **ruoli** differenti."
      ],
      "metadata": {
        "id": "kUNiRv4f7KeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa nuova chain generata dalle funzioni *built-in*, si aspetta quindi in input un **dizionario** che contiene la chiave \"**`input`**\" valorizzata con la query e restituisce in output un **dizionario** in cui la stringa di risposta si trova come valore associato ad una chiave chiamata \"**`answer`**\".\n",
        "\n",
        "Per invocare la chain e visualizzare il risultato mediante streaming possiamo farlo nel modo seguente.\n"
      ],
      "metadata": {
        "id": "Lgv3ta0P2npz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream({\"input\": \"Come imposto l'esposizione matrix?\"}):\n",
        "    if \"answer\" in chunk: # Poiché uso lo streaming la chiave answer compare dopo alcuni chunk per cui è necessario il test\n",
        "      print(chunk[\"answer\"], end=\"\", flush=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "syGgm5uY6e8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel seguito una alternativa di esecuzione senza streaming.\n"
      ],
      "metadata": {
        "id": "zIO0SaIe8iRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"come imposto l'esposizione matrix?\"})\n",
        "\n",
        "\n",
        "# stampo la risposta con il corretto formato\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import output\n",
        "\n",
        "display(Markdown(response[\"answer\"]))"
      ],
      "metadata": {
        "id": "k6SEoAW70IZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sezione 3 - Costruzione di un ChatBot Rag conversazionale"
      ],
      "metadata": {
        "id": "EKdz33BOG-IQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il ChatBot realizzato nelle sezione precedente risponde esclusivamente alla **query** fornita al momento dell'invocazione senza tenere in considerazione eventuali altri messaggi e risposte scambiati in precedenza.\n",
        "\n",
        "Un **ChatBot conversazionale**, al contrario, è un assistente in grado di rispondere tenedo in considerazione anche eventuali messaggi scambiati in precedenza.\n",
        "\n",
        "Un assistente di questo tipo deve quindi necessariamente operare mantenendo attive delle **sessioni**. La prima volta che l'utente richiede assistenza formulando una domanda, deve essere inizializzata una nuova **sessione** in modo che tutte le successive richieste avvengano nell'ambito di questa specifica sessione.\n",
        "\n",
        "In uno **scenario di produzione** deve essere definito il meccanismo per la gestione delle **sessioni**: quando iniziare una nuova sessione, a cosa legare la sessione (ad esempio associare la sessione della chat di assistenza alla sessione di accesso dell'utente al prodotto), quando chiudere una sessione (per iniziativa dell'utente o perché l'assistente ritiene che l'ultima risposta fornita sia definitiva o per timeout, ...\n",
        "\n",
        "Nel caso del nostro prototipo ci limiteremo ad impostare un semplice meccanismo per identificare la sessione corrente, a puro scopo esemplificativo.\n",
        "\n",
        "Tutti i messaggi, **query** e **risposte** scambiati durante una sessione costituiscono una **chat history**.\n",
        "\n",
        "Un **ChatBot Conversazionale** dovrà quindi mantenere uno stato che terrà conto di **session ID** e **chat history**.\n",
        "\n",
        "Nella definizione dei **prompt** occorrerà tenere conto del fatto che una **query** posta dall'utente possa implicitamente fare riferimento a messaggi o risposte presenti nella **chat history**.\n",
        "\n",
        "Lo schema **RAG** ne risulta particolarmente affetto, in quanto un'eventuale **query**, la cui interpretazione dipenda dal contesto, non è immediatamente utilizzabile per la **ricerca semantica**.\n",
        "\n",
        "Un esempio potrebbe essere la seguente query: \"*Come ne inserisco una?*\".\n",
        "\n",
        "**A cosa si riferisce l'utente?**\n",
        "\n",
        "In un contesto in cui la **chat history** fosse la seguente:\n",
        "\n",
        "- ***Utente***: *Il sistema gestisce la possibilità di creare più di una utenza?*\n",
        "\n",
        "- ***Assistente***: *Si, certamente. L'amministratore può definire utenze diverse con differenti profili di sicurezza.*\n",
        "\n",
        "la **query** andrebbe interpretata: \"*come posso inserire una nuova utenza?*\"\n",
        "\n",
        "Al contrario, nel caso in cui la **chat history** fosse la seguente:\n",
        "\n",
        "- ***Utente***: *Si possono gestire le gare?*\n",
        "\n",
        "- ***Assistente***: *Si, certamente. Il sistema permette la difinizione di gare con diverse modalità di svolgimento.....*\n",
        "\n",
        "la **query** andrebbe interpretata: \"*come posso inserire una nuova gara?*\"\n",
        "\n",
        "Pertanto, per gestire l'operazione di **retreival** dobbiamo per prima cosa costruire un tipo di **retreiver** che tenga conto della **chat history**.\n",
        "\n",
        "Il flusso delle operazioni diviene quindi leggermente più complicato.\n",
        "\n",
        "Possiamo sfruttare il *LLM* per riformulare la *query* ambigua in modo non ambiguo e quindi usare questa nuova query per effettuare l'operazione di *retreival*.\n",
        "\n",
        "**LangChain** offre una potente classe, specificamente progettata per gestire problemi di questo tipo.\n",
        "\n",
        "La cella che segue mostra come realizzare un **retreiver** che tiene conto della **chat history** utilizzando il metodo LangChain **create_history_aware_retriever** e la tecnica appena descritta.\n",
        "\n",
        "Per prima cosa viene creato uno specifico **prompt template** per chiedere al **LLM** di creare una nuova **query** che sia significativa senza dover ricorrere alla **chat history**.\n",
        "\n",
        "Quindi, viene creato l'oggetto **`history_aware_retriever`** passando al metodo **create_history_aware_retriever** l'oggetto **llm**, che rappresenta il client di accesso al modello LLM che abbiamo selezionato, l'oggetto **retreiver**, che è il *retreiver* classico recuperato dal nostro **vectorstore** e il **prompt template** che contiene le istruzioni per il riformulamento della **query**.\n",
        "\n",
        "Si noti che il **prompt template** che abbiamo definito contiene un **placeholder** per l'inserimento dell'intera **chat history**.\n",
        "\n",
        "Per consentire la costruzione del template attraverso l'impiego del **placeholder** è stato usato il metodo LangChain **ChatPromptTemplate.from_messages**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5-UQQapgtQqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# Testo del prompt di sistema che istruisce il LLM su come riformulare la query\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Data la chat history e l'ultima query dell'utente,\"\n",
        "    \"che potrebbe anche fare riferimento al contesto nella chat history, \"\n",
        "    \"genera una nuova query che possa essere compresa \"\n",
        "    \"senza la chat history. NON rispondere alla domanda, \"\n",
        "    \"ma riformula la domanda, se necessario, oppure restituiscila così come è.\"\n",
        ")\n",
        "\n",
        "# Definizione del prompt template per ottenere la query contestualizzata\n",
        "# MessagePlaceholder prevede che venga inserito in quel punto del template una sequenza di messaggi\n",
        "# associati alla chiave 'chat_history'\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n"
      ],
      "metadata": {
        "id": "Kk01d1H5BCtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ovviamente, anche il **prompt template** che useremo per invocare il LLM chiedendo di rispondere alla **query** dell'utente dovrà essere modificato per inserire la **chat history**.\n",
        "\n",
        "Nella cella che segue, viene costruito il nuovo **prompt template**, usando il metodo **ChatPromptTemplate.from_messages** a cui passiamo come argomenti:\n",
        "\n",
        "- Un template di messaggio di sistema che prevede l'inserimento di una variabile **context**\n",
        "- Un **placeholder** per l'inserimento della **chat_history**\n",
        "- Un template di messaggio utente che prevede l'inserimento di una variabile **input**\n",
        "\n"
      ],
      "metadata": {
        "id": "Vc8F60R82WvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"Sei un assistente che aiuta l'utente a trovare soluzioni alle procedure di uso del prodotto. \"\n",
        "    \"Usa come contesto i contenuti recuperati dal manuale utente. \"\n",
        "    \"Per la risposta usa anche le tue competenze generali,\"\n",
        "    \"Se ritieni che nel contesto fornito le informazioni siano molto insufficienti, consiglia di rivolgersi ad un assistente umano.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PZsruQXURtSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto, con due istruzioni, possiamo creare la **chain** utilizzando le consuete funzioni **built-in** offerte da LangChain."
      ],
      "metadata": {
        "id": "Tw6NpNao4wSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "MHd5nqqC49NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gestione della chat history e della sessione\n",
        "\n",
        "Fino a questo punto abbiamo sempre fatto riferimento alla **chat history** attraverso il nome del suo segnaposto: **chat_history**.\n",
        "\n",
        "Come implementiamo fisicamente questa entità?\n",
        "\n",
        "Come interviene il concetto di sessione?\n",
        "\n",
        "In un contesto multi utente e multi threading, le chat history devono essere riferite a singoli thread o sessioni.\n",
        "\n",
        "Un modo semplice e diretto di prototipare questo aspetto è quello di utilizzare un dizionario Python, definendo delle chiavi corrispondenti ai singoli identificatori di sessione e associare a tali chiavi la sequenza di messaggi che si riferisce a quella particolare sessione.\n",
        "\n",
        "LangChain offre una serie di classi molto utili per gestire le chat history.\n",
        "\n",
        "Nel codice seguente sfruttiamo queste classi, mostrandone l'utilizzo.\n",
        "\n",
        "Per prima cosa viene inizializzato il dizionario chiamato **store** come dizionario vuoto.\n",
        "\n",
        "Viene quindi definita una funzione, chiamata **get_session_history** che riceve una stringa come parametro di input, corrispondente ad un identificatore di sessione, e restituisce un oggetto della classe LangChain **BaseMessageHistory**.\n",
        "\n",
        "**BaseMessageHistory** è una classe astratta di LangChain che specifica l'interfaccia generale degli oggetti che costituiscono strutture idonee a memorizzare cronologie di messaggi. L'interfaccia prevede metodi standard per aggiungere e rimuovere messaggi dalla cronologia.\n",
        "\n",
        "La funzione accede alla variabile globale **store* e verifica se la stringa in input è presente come chiave nel dizionario.\n",
        "\n",
        "Se non è presente, viene creata con tale stringa una chiave nuova nel dizionario, a cui viene associato un nuovo oggetto della classe **ChatMessageHistory**.\n",
        "\n",
        "La classe **ChatMessageHistory** è una semplice implementazione della classe astratta **BaseMessageHistory** che mantiene in memoria la cronologia dei messaggi.\n",
        "\n",
        "*In uno scenario di produzione avremmo usato una implementazione di tipo persistente, ad esempio avremmo potuto usare la **SqlChatMessageHistory** che memorizza la cronologia di messaggi in un DB SQLLite.*\n",
        "\n",
        "Infine, il valore associato alla chiave passata come parametro di input viene restituito.\n",
        "\n",
        "In pratica la funzione restituisce l'oggetto che offre i metodi previsti dall'interfaccia astratta standard di LangChain per gestire la memorizzazione della chat history associata all'ID di sessione.\n",
        "\n",
        "A questo punto interviene l'utilissima classe **RunnableWhithMessageHistory** che consente di trasformare una **chain** relativa a un ChatBot in una nuova **chain** che gestisce in modo corretto la **chat history**.\n",
        "\n",
        "I parametri da fornire al costruttore **RunnableWithMessageHistory** sono:\n",
        "\n",
        "- la **chain** di partenza\n",
        "- la funzione da usare per recuperare l'oggetto **BaseChatMessageHistory** partendo dall'ID di sessione (la funzione *get_session_history* è stata definita proprio per questo scopo)\n",
        "- La chiave che identifica il messaggio di input, cioè la *query* dell'utente\n",
        "- La chiave che identifica il *placeholder* della chat history\n",
        "- La chiave che identifica il messaggio di output, cioè la risposta all'utente\n",
        "\n",
        "In questo modo viene creata la nuova **chain** **conversational_rag_chain** che gestisce sessioni e chat history.\n",
        "\n",
        "Questa nuova chaincomprende tutte le operazioni necessarie a gestire la cronologia dei messaggi. In particolare si occupa di recuperare la cronologia dei messaggi per poterli fornire al primo elemento della chain di base e al termine della esecuzione della chain di base aggiunge la risposta fornita alla cronologia dei messaggi.\n",
        "\n",
        "La **conversational_rag_chain**, come tutti i **runnable** generati tramite il metodo **RunnableWhithMessageHistory** prevedono che venga passato un parametro di configurazione corrispondente alla chiave predefinita \"session_id\" con cui recuperare la **BaseChatMessageHistory** associata alla specifica sessione.\n",
        "\n"
      ],
      "metadata": {
        "id": "cM3vC1G8D35V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "eVkefp1aC7Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per provare il funzionamento del nuovo **ChatBot RAG conversazionale** possiamo usare il codice seguente, che crea un loop consentendo all'utente di inserire più domande, costruendo una conversazione.\n",
        "\n",
        "La chain viene invocata passando due parametri:\n",
        "\n",
        "- un dizionario contenente la chiave \"input\" valorizzata con la query inserita dall'utente\n",
        "- il parametro **config** valorizzato con un dizionario in cui la chiave \"**configurable** è associata ad un ulteriore dizionario che abbina la chiave \"**session_id**\" ad un identificativo fittizio che abbiamo arbitrariamente definito. LangChain offre la possibilità di definire parametri di configurazione personalizzati chiamati **configurable**, nel nostro caso abbiamo scelto di usare questo metodo per passare alla chain l'identificativo della sessione corrente. I *configurable* sono un'altra caratteristica, molto potente, che consente di creare chain adatte ad un contesto di produzione."
      ],
      "metadata": {
        "id": "bbqTtaFwKMOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while True:\n",
        "  domanda = input(\"> \")\n",
        "  if domanda == \"stop\":\n",
        "    break\n",
        "  risposta = conversational_rag_chain.invoke({\"input\": domanda}, config={\"configurable\": {\"session_id\": \"abc123\"}} )\n",
        "  print(\"\\n\",risposta[\"answer\"],\"\\n\")"
      ],
      "metadata": {
        "id": "-8gs1mI0EtR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avremmo potuto, ovviamente, creare la **chain** conversazionale  in modo esplicito, anziché utilizzare gli strumenti built-in di LangChain, ma il codice sarebbe stato molto più complesso e il processo di sviluppo più lungo.\n",
        "\n",
        "L'uso del framework permette al progettista di concentrarsi sugli aspetti che contano e che sono distintivi delle tecniche di utilizzo dei LLM."
      ],
      "metadata": {
        "id": "BNMuhvQeb947"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sezione 4 - Realizzazione di un agente"
      ],
      "metadata": {
        "id": "fhzlBGVFGQao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La soluzione fornita nella sezione precedente è già ottimale e potrebbe essere la base per confezionare il servizio da distribuire in produzione.\n",
        "\n",
        "Tuttavia, presenta alcuni potenziali svantaggi, derivanti dal fatto che il **workflow** della **chain** è fisso.\n",
        "\n",
        "Un utente potrebbe esordire nella sua conversazione con un messaggio generico del tipo: \"*Ciao, sono Stefano*.\"\n",
        "\n",
        "Se proviamo ad inviare un tale messaggio alla chain precedente, osserveremo che il ChatBot fornirà una corretta risposta di circostanza del tipo: \"*Ciao Stefano, come posso aiutarti?*\", tuttavia, conoscendo il flusso che abbiamo implementato, la nostra chain avrà comunque effettuato una operazione di **retrieval**, che implica l'invocazione del servizio di **embeddings** che ha un costo, che invece, avrebbe potuto essere evitato.\n",
        "Quando poi viene invocato il LLM per la risposta finale, sarà inviato in input anche l'inutile contesto recuperato dal *retreiver*, con aggravio di token e quindi di costo.\n",
        "\n",
        "Inoltre, se otteniamo una risposta di circostanza corretta, ciò è dovuto alla buona qualità raggiunta dai recenti LLM che non si lasciano confondere dal contesto non pertinente alla query. Un modello meno performante avrebbe potuto generare una risposta più bizzarra.\n",
        "\n",
        "Come possiamo risolvere questo tipo di problemi?\n",
        "\n",
        "Un metodo è quello di creare un flusso di elaborazione più articolato, che preveda delle possibili diramazioni condizionali.\n",
        "\n",
        "Potremmo verificare il tipo di query inserita e decidere di non effettuare il *retreival* se questo non risulti necessario.\n",
        "\n",
        "Ma come può il nostro software analizzare la query per prendere questa decisione?\n",
        "\n",
        "Potremmo utilizzare proprio le capacità di ragionamento del LLM per effettuare questa valutazione.\n",
        "\n",
        "Quello a cui stiamo pensando è un software il cui flusso di elaborazioni ha delle diramazioni determinate dalla risposta di un LLM, e quindi in ultima analisi, dalla sua capacità di ragionamento.\n",
        "\n",
        "Un sistema di questo tipo è definito **agente**.\n",
        "\n",
        "L'implementazione di **agenti** pone una serie di problemi e, sebbene sia possibile anche usando sistemi LLM di generazione precedente, è una tecnica possibile soprattutto con i Chat LLM di ultima generazione che prevedono, e sono quindi stati addestrati per prevedere, l'impiego di **tool**.\n",
        "\n",
        "Un modello di questo tipo, come quello utilizzato in questo prototipo, è in grado di valutare se una *query* possa essere immediatamente gestita, oppure se sia più utile invocare uno o più **tool**, tra quelli messi a disposizione, per recuperare ulteriori informazioni.\n",
        "\n",
        "Tramite il potente formalismo di LangChain è possibile sviluppare un flusso di elaborazione di diverse **chain** articolate tra loro, tuttavia, questa strada richiede una buona conoscenza del problema e una certa dose di lavoro.\n",
        "\n",
        "In realtà il settore della ricerca in questo campo ha già prodotto delle *best practices* ottimizzate per i diversi LLM presenti sul mercato e LangChain offre un supporto potente e specifico per la creazione degli agenti.\n",
        "\n",
        "In particolare, il framework aggiuntivo **LangGraph** è stato rilasciato proprio per agevolare lo sviluppo di flussi di elaborazione di tipo **agentico**.\n",
        "\n",
        "Nel seguito costruiremo una **soluzione agentica** al nostro problema utilizzando questi strumenti."
      ],
      "metadata": {
        "id": "PSEQAwX9cqDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trasformare il *retreiver* in un tool\n",
        "\n",
        "Per realizzare un processo **RAG** tramite un agente dobbiamo trasformare il nostro **retreiver** in un **tool**.\n",
        "\n",
        "Ogni moderno Chat LLM prevede la possibilità di fornire nel *prompt* una lista di **tool**.\n",
        "\n",
        "L'interfaccia è diversa per ogni LLM, sia nelle modalità di passaggio della lista dei tool sia nella struttura stessa di questo parametro e degli stessi tool.\n",
        "\n",
        "I **tool** sono funzioni o API, locali o remote, che devono avere una interfaccia standard e una descrizione in linguaggio naturale, ovviamente ogni LLM ha le sue peculiari specifiche, in modo che il LLM possa decidere se richiederne l'invocazione prima di fornire una risposta definitiva alla query dell'utente.\n",
        "\n",
        "LangChain fornisce un modello astratto ed elegante per tutto questo, che ci permette di definire **tool** ed **agenti** in modo indipendente dalle specifiche indotte dal particolare LLM che andremo ad utilizzare.\n",
        "\n",
        "Il **retreiver**, quindi, non sarà sempre invocato, in quanto passaggio fisso all'interno di una **chain**, ma sarà un **tool** a disposizione del LLM che avrà facoltà di decidere se invocarlo o meno.\n",
        "\n",
        "LangChain fornisce un metodo di libreria per generare un **tool** da un **retreiver**.\n",
        "\n",
        "La cella di codice seguente crea il nostro **retreiver tool**.\n",
        "\n",
        "Come si vede, nella definizione del **tool** è necessario specificare un identificatore e una descrizione.\n",
        "\n",
        "Entrambi i campi devono essere definiti con attenzione e devono essere ampiamente descrittivi del compito e delle funzionalità assegnate al **tool**.\n",
        "\n",
        "Questi campi diventano parte del **prompt** e quindi sono usati nel **ragionamento** del LLM.\n",
        "\n",
        "Il LLM decide di utilizzare il **tool** perché dalla sua descrizione e dal suo *nome* risulta evidente che possa essere utile per elaborare la risposta!"
      ],
      "metadata": {
        "id": "dda0wbjli5fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"product_document_retriever\",\n",
        "    \"Ricerca e restituisce testi significativi dalla documentazione sul prodotto.\",\n",
        ")\n",
        "tools = [tool]\n"
      ],
      "metadata": {
        "id": "B52Q3cXysdu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione dell'agente\n",
        "\n",
        "Il nostro agente deve svolgere più o meno il seguente flusso di lavoro:\n",
        "\n",
        "- Riceve un input sotto forma di messaggi (la query iniziale dell'utente, oppure la query con una cronologia di messaggi, ed eventualmente un contesto)\n",
        "- Chiama il modello LLM per decidere se utilizzare il **retriever**\n",
        "- Se il modello decide di usare il **retriever**, lo invoca e passa i risultati nuovamente al modello aggiungendo eventualmente riformulando il prompt (questi ultimi due passi potrebbero essere ripetuti più volte)\n",
        "- Se il modello non richiede l'uso del tool, termina e risponde all'utente.\n",
        "\n",
        "Questo schema per il flusso di elaborazione è di tipo abbastanza comune ed è stato generalizzato in un modello di agente chiamato **ReAct**, dalla crasi di **Reasonining** e **Act**.\n",
        "\n",
        "Un **agente**, ovviamente, deve gestire uno stato interno che comprenderà, oltre ad un ID di sessione, la cronologia dei messaggi della chat ma anche le sequenze dei messaggi di risposta e i nuovi prompt generati nei cicli intermedi.\n",
        "\n",
        "**LangGraph** prevede una funzione **prebuild** per generare proprio un **agente** di tipo **ReAct**.\n",
        "\n",
        "Quello che serve fornire a questo metodo **prebuild** è semplicemente:\n",
        "\n",
        "- Un oggetto runnable llm\n",
        "- Una lista di tool\n",
        "- Un oggetto della classe **Checkpointer** per gestire la persistenza dello stato.\n",
        "\n",
        "Nella cella che segue viene creato l'oggetto **agent_executor** utilizzando **create_react_agent** e passando come *checkpointer* l'oggetto **memory** creato con il costruttore della classe **MemorySaver** che costituisce una semplice implementazione in memoria di un *checkpoint saver* LangGraph.\n",
        "\n",
        "Utilizziamo un **prompt di sistema** per modificare lo stato interno dell'agente **prebuild** in modo da personalizzare l'agente al nostro obiettivo."
      ],
      "metadata": {
        "id": "ryusb8WcmCZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "system_prompt = (\"Sei un assistente di nome Marta. Il tuo scopo è fornire assistenza nell'uso del prodotto.\"\n",
        "                  \"Il tuo compito è ridurre gli accessi al contact center di assistenza del prodotto.\"\n",
        "                  \"Usa sempre le informazioni recuperate dai manuali.\"\n",
        "                  \"Se ti vengono poste domande che non riguardano il prodotto, la prima volta rispondi\"\n",
        "                  \"in modo sintetico usando le tue capacità e comunque ricorda all'utente chi sei e che tipo di domande puoi ricevere.\"\n",
        "                  \"Per domande riguardanti la fotografia in generale e le fotocamere nikon, puoi rispondere\"\n",
        "                  \"usando anche la tua conoscenza generale.\"\n",
        "                  \"Usa sempre un tono cordiale e sii cortese.\"\n",
        "                )\n",
        "memory = MemorySaver()\n",
        "agent_executor = create_react_agent(llm, tools, checkpointer=memory, state_modifier =system_prompt)\n"
      ],
      "metadata": {
        "id": "013uqygTzRZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella cella che segue è contenuto il codice per provare il funzionamento del nostro **agente**.\n",
        "\n",
        "L'input atteso dall'agente creato mediante il **prebuild** di **LangGraph** è costituito da un dizionario contenente almeno la chiave \"**messages**\" che viene valorizzata con i messaggi di **input**.\n",
        "\n",
        "Nel nostro caso, la struttura dell'input è costituita dalla sola **query** dell'utente, per cui utilizziamo la classe helper **HumanMessage** di LangChain per creare la struttura del messaggio utente.\n",
        "\n",
        "In aggiunta, viene passato al metodo invoke dell'agente un **configurable** che definisce l'identificativo di sessione. Abbiamo usato anche questa volta una stringa arbitraria.\n",
        "\n",
        "In questo caso, il nome predefinito per la chiave che specifica l'ID di sessione è \"**thread_id**\".\n",
        "\n",
        "Si noti che la struttura della risposta fornita dall'agente è un dizionario che contiene la chiave **messages**.\n",
        "\n",
        "Il valore associato a questa chiave è una lista che contiene tutti i messaggi della chat history compresi quelli generati dal LLM nei cicli intermedi.\n",
        "\n",
        "La risposta all'utente è l'ultimo messaggio di questa lista."
      ],
      "metadata": {
        "id": "ynlxzJkdqU4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "while True:\n",
        "  domanda = input(\"> \")\n",
        "  if domanda == \"stop\":\n",
        "    break\n",
        "  risposta = agent_executor.invoke({\"messages\": [HumanMessage(content=domanda)]}, config=config)\n",
        "  display(Markdown(risposta[\"messages\"][-1].content))\n",
        "  print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Te6scFnu1a3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisi del comportamento dell'agente"
      ],
      "metadata": {
        "id": "IDjIyAKiV_rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella di codice che segue permette di visualizzare lo stato dell'agente che comprende il flusso dei messaggi scambiati, comprese le risposte del LLM in cui viene chiesta l'invocazione del tool."
      ],
      "metadata": {
        "id": "E6Wpr3xDVoip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in risposta[\"messages\"]:\n",
        "  print (msg.pretty_print(),\"\\n\")"
      ],
      "metadata": {
        "id": "zEzGQBaQLXzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}